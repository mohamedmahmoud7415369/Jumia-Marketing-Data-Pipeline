# ðŸ“š Jumia Marketing Data Pipeline - Complete Technical Documentation

**Version:** 1.0  
**Last Updated:** 2025-11-26  
**Author:** Data Engineering Team

---

## ðŸ“– Table of Contents

1. [Project Overview](#1-project-overview)
2. [System Architecture](#2-system-architecture)
3. [Infrastructure Files](#3-infrastructure-files)
4. [DAG Files](#4-dag-files)
5. [Source Code Files](#5-source-code-files)
6. [Dashboard Files](#6-dashboard-files)
7. [Data Flow](#7-data-flow)
8. [Deployment Guide](#8-deployment-guide)
9. [Troubleshooting](#9-troubleshooting)

---

## 1. Project Overview

### 1.1 Purpose
The Jumia Marketing Data Pipeline is an end-to-end automated data pipeline that:
- **Scrapes** product data from Jumia Egypt marketplace
- **Processes** and transforms the data into a dimensional model
- **Stores** data in a DuckDB data warehouse
- **Generates** marketing insights and analytics
- **Visualizes** insights through an interactive dashboard

### 1.2 Technology Stack
- **Orchestration**: Apache Airflow
- **Scraping**: Selenium WebDriver
- **Database**: DuckDB (embedded analytical database)
- **Data Processing**: Pandas, Python
- **Visualization**: Streamlit, Plotly
- **Containerization**: Docker, Docker Compose
- **Language**: Python 3.9+

### 1.3 Project Structure
```
Jumia_Marketing_Data_Pipline/
â”œâ”€â”€ dags/                      # Airflow DAG definitions
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â””â”€â”€ jumia_pipeline.py      # Main orchestration DAG
â”œâ”€â”€ src/                       # Source code modules
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ jumia_scraper.py       # Web scraping logic
â”‚   â”œâ”€â”€ create_schema.py       # Database schema creation
â”‚   â”œâ”€â”€ etl_pipeline.py        # ETL processing logic
â”‚   â””â”€â”€ generate_insights.py  # Marketing insights generation
â”œâ”€â”€ dashboard/                 # Interactive dashboard
â”‚   â”œâ”€â”€ app.py                 # Streamlit dashboard application
â”‚   â”œâ”€â”€ config.py              # Dashboard configuration
â”‚   â”œâ”€â”€ utils.py               # Dashboard utilities
â”‚   â””â”€â”€ README.md              # Dashboard documentation
â”œâ”€â”€ data/                      # Data storage
â”‚   â”œâ”€â”€ raw/                   # Raw scraped CSV files
â”‚   â””â”€â”€ jumia_warehouse.duckdb # DuckDB database
â”œâ”€â”€ logs/                      # Airflow and application logs
â”œâ”€â”€ Docs/                      # Generated documentation
â”‚   â””â”€â”€ marketing_insights.md  # Markdown insights report
â”œâ”€â”€ docker-compose.yaml        # Docker services configuration
â”œâ”€â”€ Dockerfile                 # Airflow container image
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ reset_project.ps1          # Project reset script
```

---

## 2. System Architecture

### 2.1 High-Level Architecture

```mermaid
graph TB
    A[Jumia Website] -->|Selenium Scraper| B[Raw CSV Files]
    B -->|ETL Pipeline| C[DuckDB Warehouse]
    C -->|Insights Generator| D[Markdown Report]
    C -->|Dashboard Queries| E[Streamlit Dashboard]
    
    F[Airflow Scheduler] -->|Orchestrates| G[DAG Tasks]
    G -->|Executes| H[Scraper]
    G -->|Executes| I[ETL]
    G -->|Executes| J[Insights]
    
    K[Docker Compose] -->|Manages| L[Airflow Services]
    K -->|Manages| M[PostgreSQL]
    K -->|Manages| N[Selenium Grid]
```

### 2.2 Component Interaction

| Component | Depends On | Produces | Consumed By |
|-----------|------------|----------|-------------|
| **Web Scraper** | Selenium Grid | Raw CSV files | ETL Pipeline |
| **ETL Pipeline** | Raw CSV files | DuckDB database | Insights Generator, Dashboard |
| **Schema Creator** | None | Database schema | ETL Pipeline |
| **Insights Generator** | DuckDB database | Markdown report | Business users |
| **Dashboard** | DuckDB database | Interactive visualizations | Business users |
| **Airflow** | PostgreSQL | Task orchestration | All pipeline components |

### 2.3 Data Model

**Dimensional Model (Star Schema)**

```
fact_product_daily
â”œâ”€â”€ fact_id (PK)
â”œâ”€â”€ product_key (FK â†’ dim_product)
â”œâ”€â”€ date_key (FK â†’ dim_date)
â”œâ”€â”€ price_egp
â”œâ”€â”€ old_price_egp
â”œâ”€â”€ discount_pct
â”œâ”€â”€ rating
â”œâ”€â”€ review_count
â””â”€â”€ scrape_timestamp

dim_product
â”œâ”€â”€ product_key (PK)
â”œâ”€â”€ product_url (Natural Key)
â”œâ”€â”€ product_name
â”œâ”€â”€ brand
â”œâ”€â”€ category
â”œâ”€â”€ image_url
â”œâ”€â”€ is_official
â”œâ”€â”€ is_express
â”œâ”€â”€ effective_date
â”œâ”€â”€ expiration_date
â””â”€â”€ is_current

dim_date
â”œâ”€â”€ date_key (PK)
â”œâ”€â”€ full_date
â”œâ”€â”€ day_name
â”œâ”€â”€ day_of_week
â”œâ”€â”€ day_of_month
â”œâ”€â”€ month_name
â”œâ”€â”€ month
â”œâ”€â”€ year
â””â”€â”€ is_weekend
```

---

## 3. Infrastructure Files

### 3.1 docker-compose.yaml

**Purpose:** Defines and manages all Docker services for the project.

#### Line-by-Line Explanation

```yaml
version: '3.8'  # Docker Compose file format version
```
- Specifies Docker Compose v3.8 for compatibility

```yaml
services:
  postgres:
    image: postgres:13
```
- **Service Name:** `postgres`
- **Purpose:** Metadata database for Airflow
- **Image:** PostgreSQL 13 (stable, well-supported)

```yaml
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
```
- **POSTGRES_USER:** Database username (airflow)
- **POSTGRES_PASSWORD:** Database password (airflow)
- **POSTGRES_DB:** Initial database name (airflow)
- **Effect:** Creates Airflow's metadata database

```yaml
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
```
- **Volume Mount:** Persists PostgreSQL data between container restarts
- **Location:** Named volume `postgres-db-volume`
- **Target:** `/var/lib/postgresql/data` (PostgreSQL data directory)

```yaml
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
```
- **Healthcheck:** Tests if PostgreSQL is ready to accept connections
- **Command:** `pg_isready -U airflow`
- **Interval:** Check every 5 seconds
- **Retries:** Up to 5 attempts before marking unhealthy
- **Effect:** Other services wait for PostgreSQL to be healthy before starting

```yaml
  fix-permissions:
    image: alpine
    command: sh -c "mkdir -p /opt/airflow/data /opt/airflow/logs && chmod -R 777 /opt/airflow/data /opt/airflow/logs"
    user: "0:0"
```
- **Service Name:** `fix-permissions`
- **Purpose:** One-time setup to fix directory permissions
- **Image:** Alpine Linux (minimal, fast)
- **Command:** Creates data/logs directories with full permissions
- **User:** Root (UID:GID 0:0) for permission management
- **Effect:** Prevents permission errors when containers write files

```yaml
    volumes:
      - ./data:/opt/airflow/data
      - ./logs:/opt/airflow/logs
```
- **Volume Mounts:** Binds host directories to container
- `./data` â†’ `/opt/airflow/data` (persistent data storage)
- `./logs` â†’ `/opt/airflow/logs` (log files)

```yaml
  airflow-init:
    build: .
    command: version
```
- **Service Name:** `airflow-init`
- **Purpose:** Initialize Airflow database and create admin user
- **Build:** Uses local Dockerfile to build image
- **Command:** `airflow version` (triggers database initialization)

```yaml
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
```
- **Database Connection String:**
  - Protocol: `postgresql+psycopg2`
  - User: `airflow`
  - Password: `airflow`
  - Host: `postgres` (service name resolves to container IP)
  - Database: `airflow`

```yaml
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
```
- **_AIRFLOW_DB_UPGRADE:** Runs database schema migrations
- **_AIRFLOW_WWW_USER_CREATE:** Creates web UI user
- **_AIRFLOW_WWW_USER_USERNAME:** Admin username (admin)
- **_AIRFLOW_WWW_USER_PASSWORD:** Admin password (admin)
- **Effect:** Sets up Airflow database and admin credentials

```yaml
    depends_on:
      postgres:
        condition: service_healthy
      fix-permissions:
        condition: service_completed_successfully
```
- **Dependency Management:**
  - Waits for PostgreSQL to be healthy
  - Waits for permissions fix to complete
- **Effect:** Ensures proper startup order

```yaml
  airflow-webserver:
    build: .
    command: webserver
    ports:
      - "8080:8080"
```
- **Service Name:** `airflow-webserver`
- **Purpose:** Airflow web UI server
- **Command:** Starts the webserver process
- **Ports:** Maps host port 8080 to container port 8080
- **Access:** http://localhost:8080

```yaml
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
```
- **Executor Type:** LocalExecutor (runs tasks in separate processes)
- **Alternatives:** SequentialExecutor (one task at a time), CeleryExecutor (distributed)
- **Effect:** Enables parallel task execution on single machine

```yaml
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
```
- **Fernet Key:** Encryption key for sensitive data (connections, variables)
- **Format:** Base64-encoded 32-byte key
- **Effect:** Secures passwords and tokens in Airflow database

```yaml
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
```
- **Load Examples:** Disabled (prevents example DAGs from loading)
- **Effect:** Cleaner DAG list with only project DAGs

```yaml
      - AIRFLOW_ADMIN_USER=admin
      - AIRFLOW_ADMIN_PASSWORD=admin
      - SELENIUM_REMOTE_URL=http://selenium:4444/wd/hub
```
- **Admin Credentials:** Username and password for Airflow UI
- **Selenium URL:** Remote WebDriver endpoint
  - Protocol: HTTP
  - Host: `selenium` (Docker service name)
  - Port: 4444
  - Path: `/wd/hub` (Selenium Grid endpoint)
- **Effect:** Enables scraper to use remote Selenium browser

```yaml
    healthcheck:
      test: ["CMD-SHELL", "[ -f /opt/airflow/airflow-webserver.pid ]"]
      interval: 30s
      timeout: 30s
      retries: 3
```
- **Healthcheck:** Verifies webserver PID file exists
- **Interval:** Check every 30 seconds
- **Timeout:** 30 seconds per check
- **Retries:** 3 attempts before marking unhealthy

```yaml
  airflow-scheduler:
    build: .
    command: scheduler
```
- **Service Name:** `airflow-scheduler`
- **Purpose:** Monitors DAGs and triggers task execution
- **Command:** Starts the scheduler process
- **Effect:** Enables automatic DAG execution

```yaml
  selenium:
    image: selenium/standalone-chrome
    ports:
      - "4444:4444"  # WebDriver endpoint
      - "7900:7900"  # VNC viewer (browser automation UI)
    shm_size: "2g"
```
- **Service Name:** `selenium`
- **Purpose:** Headless Chrome browser for web scraping
- **Image:** Official Selenium standalone Chrome
- **Port 4444:** WebDriver protocol endpoint
- **Port 7900:** VNC server for viewing browser (http://localhost:7900)
- **shm_size:** Shared memory size (2GB prevents Chrome crashes)

```yaml
volumes:
  postgres-db-volume:
```
- **Named Volume:** Persistent storage for PostgreSQL data
- **Effect:** Data survives container deletion

---

### 3.2 Dockerfile

**Purpose:** Builds custom Airflow image with project dependencies.

#### Line-by-Line Explanation

```dockerfile
FROM apache/airflow:2.5.0-python3.9
```
- **Base Image:** Official Apache Airflow 2.5.0 with Python 3.9
- **Effect:** Provides Airflow runtime environment

```dockerfile
USER root
```
- **Switch to Root:** Needed for installing system packages
- **Effect:** Grants elevated privileges for next commands

```dockerfile
RUN apt-get update && apt-get install -y --no-install-recommends \
    wget \
    gnupg \
    && rm -rf /var/lib/apt/lists/*
```
- **apt-get update:** Refreshes package index
- **apt-get install:**
  - `wget`: Download utility
  - `gnupg`: GNU Privacy Guard (for package verification)
- **--no-install-recommends:** Minimal install (reduces image size)
- **rm -rf /var/lib/apt/lists/*:** Cleans up package cache
- **Effect:** Installs required system tools, keeps image small

```dockerfile
USER airflow
```
- **Switch to Airflow User:** Returns to non-root user
- **Effect:** Improves security (least privilege principle)

```dockerfile
COPY requirements.txt /tmp/requirements.txt
```
- **Copy File:** Transfers requirements.txt to container
- **Destination:** `/tmp/requirements.txt`
- **Effect:** Makes Python dependencies available for installation

```dockerfile
RUN pip install --no-cache-dir --user -r /tmp/requirements.txt
```
- **pip install:** Installs Python packages
- **--no-cache-dir:** Doesn't store downloaded packages(reduces size)
- **--user:** Installs in user directory (no root needed)
- **-r /tmp/requirements.txt:** Reads dependencies from file
- **Effect:** Installs all project Python dependencies

---

### 3.3 requirements.txt

**Purpose:** Lists all Python package dependencies.

#### Line-by-Line Explanation

```
pandas
```
- **Package:** pandas
- **Purpose:** Data manipulation and analysis
- **Used In:** ETL pipeline, insights generation
- **Effect:** Provides DataFrames for data processing

```
selenium
```
- **Package:** Selenium WebDriver
- **Purpose:** Browser automation for web scraping
- **Used In:** jumia_scraper.py
- **Effect:** Controls Chrome browser to scrape Jumia website

```
beautifulsoup4
```
- **Package:** Beautiful Soup 4
- **Purpose:** HTML parsing and extraction
- **Used In:** jumia_scraper.py
- **Effect:** Parses HTML to extract product data

```
duckdb
```
- **Package:** DuckDB
- **Purpose:** Embedded analytical database
- **Used In:** ETL pipeline, insights generation, dashboard
- **Effect:** Provides SQL database for data warehousing

```
streamlit
```
- **Package:** Streamlit
- **Purpose:** Web dashboard framework
- **Used In:** dashboard/app.py
- **Effect:** Creates interactive web UI

```
plotly
```
- **Package:** Plotly
- **Purpose:** Interactive charting library
- **Used In:** Dashboard visualizations
- **Effect:** Generates interactive charts and graphs

```
openpyxl
```
- **Package:** openpyxl
- **Purpose:** Excel file handling
- **Used In:** Potential export functionality
- **Effect:** Enables Excel file operations (if needed)

```
webdriver-manager
```
- **Package:** WebDriver Manager
- **Purpose:** Automatic WebDriver binary management
- **Used In:** jumia_scraper.py (if not using remote Selenium)
- **Effect:** Simplifies Chrome/Firefox driver setup

---

### 3.4 reset_project.ps1

**Purpose:** PowerShell script to completely reset the project to clean state.

#### Line-by-Line Explanation

```powershell
# Stop and remove containers, networks, and volumes
Write-Host "Stopping Docker containers..."
```
- **Comment:** Description of action
- **Write-Host:** Prints message to console
- **Effect:** Informs user of current step

```powershell
docker-compose down -v
```
- **docker-compose down:** Stops and removes containers
- **-v flag:** Also removes volumes (including database data)
- **Effect:** Complete teardown of all services and data

```powershell
Write-Host "Cleaning up local data..."
if (Test-Path "data") {
    Remove-Item -Path "data" -Recurse -Force
    Write-Host "Data directory removed."
}
```
- **Test-Path:** Checks if data directory exists
- **Remove-Item:** Deletes directory
  - **-Recurse:** Deletes subdirectories recursively
  - **-Force:** Bypasses confirmation prompts
- **Effect:** Removes all local data files

```powershell
Write-Host "Rebuilding Docker images..."
docker-compose build --no-cache
```
- **docker-compose build:** Rebuilds all Docker images
- **--no-cache:** Ignores build cache (fresh build)
- **Effect:** Ensures clean image rebuild

```powershell
Write-Host "Reset complete! Run 'docker-compose up -d' to start fresh."
```
- **Final Message:** Instructions for next step
- **Effect:** Guides user to start services

---

## 4. DAG Files

### 4.1 jumia_pipeline.py

**Purpose:** Airflow DAG that orchestrates the entire data pipeline.

#### Complete Code with Line-by-Line Explanation

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os
```

**Lines 1-5: Imports**
- `airflow.DAG`: DAG class for creating workflow
- `airflow.operators.python.PythonOperator`: Runs Python functions as tasks
- `datetime`, `timedelta`: Date/time handling
- `sys`, `os`: System and OS operations

```python
# Add src directory to Python path
sys.path.insert(0, '/opt/airflow/src')
```

**Lines 7-8: Python Path Configuration**
- `sys.path.insert(0, ...)`: Adds directory to start of Python import path
- `/opt/airflow/src`: Container path to source code
- **Effect:** Allows importing custom modules (jumia_scraper, etc.)

```python
from jumia_scraper import scrape_jumia
from create_schema import create_schema
from etl_pipeline import run_etl
from generate_insights import generate_markdown_report
```

**Lines 10-13: Custom Module Imports**
- Imports project-specific functions
- **scrape_jumia:** Web scraping function
- **create_schema:** Database schema creation
- **run_etl:** ETL processing
- **generate_markdown_report:** Insights generation

```python
# Define default arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}
```

**Lines 15-23: Default Arguments**
- `owner`: DAG owner (airflow)
- `depends_on_past`: Whether task depends on previous run success (False)
- `email_on_failure`: Send email if task fails (False)
- `email_on_retry`: Send email on retry (False)
- `retries`: Number of retry attempts (1)
- `retry_delay`: Wait time between retries (5 minutes)
- **Effect:** Sets common task behavior across all tasks

```python
# Create DAG
dag = DAG(
    'jumia_marketing_pipeline',
    default_args=default_args,
    description='Scrape Jumia, load to DuckDB, and generate insights',
    schedule_interval='@daily',
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['jumia', 'scraping', 'etl'],
)
```

**Lines 25-34: DAG Definition**
- `dag_id`: 'jumia_marketing_pipeline' (unique identifier)
- `default_args`: Applies defaults to all tasks
- `description`: Human-readable DAG description
- `schedule_interval`: '@daily' = runs once per day at midnight
- `start_date`: DAG becomes active from this date
- `catchup=False`: Don't backfill missed runs
- `tags`: Labels for filtering in Airflow UI
- **Effect:** Creates schedulable workflow

```python
def scrape_task():
    """Scraper wrapper with error handling"""
    try:
        scrape_jumia()
    except Exception as e:
        print(f"Scraping failed: {e}")
        raise
```

**Lines 36-42: Scrape Task Wrapper**
- **Function:** Wraps scraper with error handling
- **try-except:** Catches and logs exceptions
- **raise:** Re-raises exception to mark task as failed
- **Effect:** Provides better error logging in Airflow UI

```python
def schema_task():
    """Database schema creation wrapper"""
    try:
        create_schema()
    except Exception as e:
        print(f"Schema creation failed: {e}")
        raise
```

**Lines 44-50: Schema Task Wrapper**
- Creates database schema with error handling
- Similar pattern to scrape_task

```python
def etl_task():
    """ETL pipeline wrapper"""
    try:
        run_etl()
    except Exception as e:
        print(f"ETL failed: {e}")
        raise
```

**Lines 52-58: ETL Task Wrapper**
- Runs ETL process with error handling

```python
def insights_task():
    """Insights generation wrapper"""
    try:
        generate_markdown_report()
    except Exception as e:
        print(f"Insights generation failed: {e}")
        raise
```

**Lines 60-66: Insights Task Wrapper**
- Generates insights report with error handling

```python
# Define tasks
scrape = PythonOperator(
    task_id='scrape_jumia_data',
    python_callable=scrape_task,
    dag=dag,
)
```

**Lines 68-73: Scrape Task**
- **task_id:** Unique task identifier
- **python_callable:** Function to execute
- **dag:** Associates task with DAG
- **Effect:** Creates executable task unit

```python
schema = PythonOperator(
    task_id='create_database_schema',
    python_callable=schema_task,
    dag=dag,
)
```

**Lines 75-79: Schema Task**
- Creates database schema task

```python
etl = PythonOperator(
    task_id='run_etl_pipeline',
    python_callable=etl_task,
    dag=dag,
)
```

**Lines 81-85: ETL Task**
- Runs ETL pipeline task

```python
insights = PythonOperator(
    task_id='generate_insights_report',
    python_callable=insights_task,
    dag=dag,
)
```

**Lines 87-91: Insights Task**
- Generates insights report task

```python
# Set task dependencies
schema >> scrape >> etl >> insights
```

**Line 93-94: Task Dependencies**
- **>>operator:** Defines execution order
- **Flow:** schema â†’ scrape â†’ etl â†’ insights
- **Effect:** Tasks run sequentially in specified order

**Complete Task Flow:**
1. Create database schema
2. Scrape Jumia website
3. Run ETL to load data into warehouse
4. Generate insights report

---

## 5. Source Code Files

### 5.1 jumia_scraper.py

**Purpose:** Web scraping module that extracts product data from Jumia Egypt.

*(Due to length constraints, I'll create this as a separate documentation file)*

---

### 5.2 create_schema.py

**Purpose:** Creates the dimensional model schema in DuckDB.

#### Complete Code with Line-by-Line Explanation

```python
import duckdb
import os
```

**Lines 1-2: Imports**
- `duckdb`: DuckDB database library
- `os`: Operating system interface

```python
def create_schema(db_path='data/jumia_warehouse.duckdb'):
    """Initialize the DuckDB schema for the Jumia Data Warehouse."""
```

**Lines 4-5: Function Definition**
- **Function:** create_schema
- **Parameter:** db_path (default: 'data/jumia_warehouse.duckdb')
- **Purpose:** Creates dimensional model tables

```python
    # Ensure data directory exists
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
```

**Lines 7-8: Directory Creation**
- `os.path.dirname(db_path)`: Gets directory part of path ('data')
- `os.makedirs(..., exist_ok=True)`: Creates directory if doesn't exist
- **Effect:** Prevents file not found errors

```python
    con = duckdb.connect(db_path)
```

**Line 10: Database Connection**
- Connects to DuckDB database file
- **Effect:** Creates file if doesn't exist

```python
    # 1. Dimension: Date
    con.execute("""
    CREATE TABLE IF NOT EXISTS dim_date (
        date_key INTEGER PRIMARY KEY,
        full_date DATE,
        day_name VARCHAR,
        day_of_week INTEGER,
        day_of_month INTEGER,
        month_name VARCHAR,
        month INTEGER,
        year INTEGER,
        is_weekend BOOLEAN
    );
    """)
```

**Lines 12-25: Date Dimension Table**
- **Table Name:** dim_date
- **Primary Key:** date_key (integer format: YYYYMMDD)
- **Columns:**
  - `full_date`: Actual date value
  - `day_name`: Monday, Tuesday, etc.
  - `day_of_week`: 1-7
  - `day_of_month`: 1-31
  - `month_name`: January, February, etc.
  - `month`: 1-12
  - `year`: 4-digit year
  - `is_weekend`: True/False flag
- **Purpose:** Enables time-based analysis and grouping

```python
    # 2. Dimension: Product (SCD Type 2 support fields included)
    con.execute("""
    CREATE TABLE IF NOT EXISTS dim_product (
        product_key INTEGER PRIMARY KEY, -- Surrogate Key
        product_url VARCHAR, -- Natural Key
        product_name VARCHAR,
        brand VARCHAR,
        category VARCHAR,
        image_url VARCHAR,
        is_official BOOLEAN,
        is_express BOOLEAN,
        effective_date DATE,
        expiration_date DATE,
        is_current BOOLEAN
    );
    """)
```

**Lines 27-42: Product Dimension Table**
- **Table Name:** dim_product
- **Primary Key:** product_key (surrogate key)
- **Natural Key:** product_url (business identifier)
- **SCD Type 2 Fields:**
  - `effective_date`: When record became active
  - `expiration_date`: When record expires
  - `is_current`: Flag for current record
- **Attributes:**
  - `product_name`: Product title
  - `brand`: Manufacturer/brand name
  - `category`: Product category
  - `image_url`: Product image link
  - `is_official`: Official brand store flag
  - `is_express`: Express delivery flag
- **Purpose:** Tracks product history over time

```python
    # 3. Fact: Product Daily Snapshot
    con.execute("""
    CREATE TABLE IF NOT EXISTS fact_product_daily (
        fact_id INTEGER PRIMARY KEY, -- Optional surrogate for fact
        product_key INTEGER,
        date_key INTEGER,
        price_egp DOUBLE,
        old_price_egp DOUBLE,
        discount_pct DOUBLE,
        rating DOUBLE,
        review_count INTEGER,
        scrape_timestamp TIMESTAMP,
        FOREIGN KEY (product_key) REFERENCES dim_product(product_key),
        FOREIGN KEY (date_key) REFERENCES dim_date(date_key)
    );
    """)
```

**Lines 44-59: Fact Table**
- **Table Name:** fact_product_daily
- **Primary Key:** fact_id (surrogate key)
- **Foreign Keys:**
  - `product_key` â†’ dim_product
  - `date_key` â†’ dim_date
- **Measures:**
  - `price_egp`: Current price in Egyptian Pounds
  - `old_price_egp`: Original price before discount
  - `discount_pct`: Discount percentage
  - `rating`: Product rating (0-5 stars)
  - `review_count`: Number of customer reviews
  - `scrape_timestamp`: When data was collected
- **Purpose:** Stores daily snapshots of product metrics

```python
    # Sequence for keys if needed (DuckDB supports sequences)
    con.execute("CREATE SEQUENCE IF NOT EXISTS product_key_seq START 1;")
    con.execute("CREATE SEQUENCE IF NOT EXISTS fact_id_seq START 1;")
```

**Lines 61-63: Sequences (Auto-increment)**
- **product_key_seq:** Generates unique product keys
- **fact_id_seq:** Generates unique fact IDs
- **START 1:** Begins at 1
- **Effect:** Provides auto-incrementing surrogate keys

```python
    print(f"Schema initialized in {db_path}")
    con.close()
```

**Lines 65-66: Cleanup**
- Prints confirmation message
- Closes database connection

```python
if __name__ == "__main__":
    create_schema()
```

**Lines 68-69: Script Entry Point**
- Executes if run directly (not imported)
- Creates schema with default path

---

### 5.3 etl_pipeline.py

**Purpose:** Extracts data from CSV files, transforms it, and loads into DuckDB warehouse.

#### Helper Functions

```python
def clean_price(price_str):
    """Clean price string to float."""
    if pd.isna(price_str) or price_str == 'N/A':
        return None
    # Remove 'EGP', commas, and whitespace
    clean = str(price_str).replace('EGP', '').replace(',', '').strip()
    try:
        return float(clean)
    except ValueError:
        return None
```

**Purpose:** Converts price strings to float
- **Input:** "1,250 EGP" or "N/A"
- **Output:** 1250.0 or None
- **Process:**
  1. Check if missing/N/A
  2. Remove 'EGP', commas, whitespace
  3. Convert to float
  4. Return None if conversion fails

```python
def clean_rating(rating_str):
    """Clean rating string to float."""
    if pd.isna(rating_str) or rating_str == 'No rating':
        return None
    try:
        return float(str(rating_str).split()[0])
    except (ValueError, IndexError):
        return None
```

**Purpose:** Converts rating strings to float
- **Input:** "4.5 out of 5" or "No rating"
- **Output:** 4.5 or None
- **Process:**
  1. Check if missing/no rating
  2. Split string and take first element
  3. Convert to float
  4. Return None if conversion fails

#### Main ETL Function

```python
def run_etl():
    """Main ETL function."""
    print("Starting ETL Process...")
```

**Function Definition**
- **Purpose:** Orchestrates entire ETL process
- Prints status message

```python
    # 1. Extract: Load all CSVs from raw directory
    all_files = glob.glob(os.path.join(RAW_DATA_DIR, "*.csv"))
    if not all_files:
        print("No raw data found.")
        return
```

**Extract Phase - File Discovery**
- `glob.glob(...)`: Finds all CSV files in raw directory
- Checks if any files exist
- Returns early if no data found

```python
    df_list = []
    for filename in all_files:
        try:
            # Check if file has content
            if os.path.getsize(filename) < 10:  # Skip files smaller than 10 bytes
                print(f"Skipping empty file: {filename}")
                continue
            df = pd.read_csv(filename)
            if not df.empty:
                df_list.append(df)
                print(f"Loaded {len(df)} records from {os.path.basename(filename)}")
            else:
                print(f"Skipping empty dataframe from: {filename}")
        except Exception as e:
            print(f"Error reading {filename}: {e}")
            continue
```

**Extract Phase - File Loading**
- Loops through each CSV file
- **Validation:**
  - Checks file size (skip if < 10 bytes)
  - Checks if DataFrame is empty
- **Error Handling:** Catches and logs errors, continues with next file
- **Effect:** Loads all valid CSV files into list

```python
    if not df_list:
        print("No valid data found in raw files.")
        return
    
    raw_df = pd.concat(df_list, ignore_index=True)
    print(f"Loaded {len(raw_df)} total raw records from {len(df_list)} files.")
```

**Extract Phase - Data Consolidation**
- Checks if any valid data loaded
- `pd.concat(...)`: Combines all DataFrames into one
- `ignore_index=True`: Renumbers rows sequentially
- Prints summary statistics

```python
    # 2. Transform
    # Clean numeric columns
    raw_df['price_egp'] = raw_df['price_egp'].apply(clean_price)
    raw_df['old_price'] = raw_df['old_price'].apply(clean_price)
```

**Transform Phase - Price Cleaning**
- Applies `clean_price` function to each price value
- Converts string prices to numeric format

```python
    # Calculate discount percentage if missing
    mask_discount = raw_df['discount'].isna() & raw_df['old_price'].notna() & raw_df['price_egp'].notna()
    raw_df.loc[mask_discount, 'discount'] = ((raw_df['old_price'] - raw_df['price_egp']) / raw_df['old_price'] * 100).round(1).astype(str) + '%'
```

**Transform Phase - Discount Calculation**
- **Condition:** Discount missing but both prices exist
- **Formula:** `((old_price - price) / old_price) * 100`
- **Example:** Old=1000, New=800 â†’ (200/1000)*100 = 20%
- **Effect:** Fills missing discount values

```python
    # Clean discount to float for analysis
    raw_df['discount_pct'] = raw_df['discount'].str.replace('%', '').str.replace('-', '').astype(float)
```

**Transform Phase - Discount Normalization**
- Removes '%' symbol
- Removes '-' character
- Converts to float
- **Example:** "20%" â†’ 20.0

```python
    raw_df['rating'] = raw_df['rating'].apply(clean_rating)
```

**Transform Phase - Rating Cleaning**
- Applies `clean_rating` function
- Converts rating strings to floats

```python
    # Handle missing values
    raw_df['review_count'] = raw_df['review_count'].fillna(0).astype(int)
    raw_df['brand'] = raw_df['brand'].fillna('Unknown')
    raw_df['is_official'] = raw_df['is_official'].fillna(False)
    raw_df['is_express'] = raw_df['is_express'].fillna(False)
```

**Transform Phase - Missing Value Handling**
- `review_count`: Fill with 0 (no reviews yet)
- `brand`: Fill with 'Unknown'
- `is_official`: Fill with False (assume non-official)
- `is_express`: Fill with False (assume standard delivery)

```python
    # Date handling
    today = datetime.now().date()
    raw_df['scrape_date'] = pd.to_datetime(raw_df['scrape_timestamp']).dt.date
```

**Transform Phase - Date Processing**
- Gets current date
- Converts timestamp to date only
- **Effect:** Enables date-based joins and grouping

```python
    # 3. Load to DuckDB
    con = duckdb.connect(DB_PATH)
```

**Load Phase - Database Connection**
- Opens connection to DuckDB warehouse

```python
    # --- Load Dim_Date (Simple generation for range) ---
    # In a real scenario, we'd generate a full calendar. Here we ensure today exists.
    # Check if date exists, if not insert
    # For simplicity in this demo, we'll just ensure the current date key exists
    date_key = int(today.strftime('%Y%m%d'))
    
    con.execute(f"""
    INSERT OR IGNORE INTO dim_date (date_key, full_date, year, month, day_of_month)
    VALUES ({date_key}, '{today}', {today.year}, {today.month}, {today.day})
    """)
```

**Load Phase - Date Dimension**
- **date_key Format:** YYYYMMDD (e.g., 20251126)
- **INSERT OR IGNORE:** Only inserts if date doesn't exist
- **Effect:** Ensures current date exists in dimension

```python
    # --- Load Dim_Product (SCD Type 1 for simplicity first, then Type 2 logic) ---
    # For this iteration, we will do a "Merge/Upsert" logic.
    # If product_url exists, update attributes. If not, insert.
    
    # We need to assign surrogate keys.
    # Let's load raw data into a temp table first
    con.register('raw_stage', raw_df)
```

**Load Phase - Staging**
- `con.register(...)`: Creates temporary table from DataFrame
- **Table Name:** 'raw_stage'
- **Purpose:** Enables SQL operations on DataFrame

```python
    # Insert new products into dim_product
    con.execute("""
    INSERT INTO dim_product (
        product_key, product_url, product_name, brand, category, 
        image_url, is_official, is_express, effective_date, is_current
    )
    SELECT 
        nextval('product_key_seq'), 
        product_url, 
        product_name, 
        brand, 
        search_term,  -- Using search_term as category
        image_url, 
        is_official, 
        is_express, 
        CURRENT_DATE, 
        TRUE
    FROM raw_stage
    WHERE product_url NOT IN (SELECT product_url FROM dim_product)
    GROUP BY product_url, product_name, brand, search_term, image_url, is_official, is_express
    """)
```

**Load Phase - Product Dimension (New Products)**
- **nextval('product_key_seq'):** Gets next surrogate key
- **WHERE clause:** Only inserts NEW products (not in dim_product)
- **GROUP BY:** Deduplicates within raw data
- **effective_date:** Set to current date
- **is_current:** TRUE (active record)
- **Effect:** Adds new products to warehouse

```python
    # --- Load Fact Table ---
    con.execute(f"""
    INSERT INTO fact_product_daily (
        fact_id, product_key, date_key, price_egp, old_price_egp, 
        discount_pct, rating, review_count, scrape_timestamp
    )
    SELECT 
        nextval('fact_id_seq'),
        dp.product_key,
        {date_key},
        rs.price_egp,
        rs.old_price,
        rs.discount_pct,
        rs.rating,
        rs.review_count,
        CAST(rs.scrape_timestamp AS TIMESTAMP)
    FROM raw_stage rs
    JOIN dim_product dp ON rs.product_url = dp.product_url
    """)
```

**Load Phase - Fact Table**
- **nextval('fact_id_seq'):** Unique fact ID
- **JOIN:** Links staging data to products via URL
- **date_key:** Uses today's date key
- **CAST:** Converts timestamp string to timestamp type
- **Effect:** Records daily product metrics

```python
    print("ETL Completed Successfully.")
    
    # Verification count
    count = con.execute("SELECT COUNT(*) FROM fact_product_daily").fetchone()[0]
    print(f"Total rows in Fact Table: {count}")
    
    con.close()
```

**Completion**
- Prints success message
- Counts total facts for verification
- Closes database connection

---

### 5.4 generate_insights.py

**Purpose:** Generates marketing insights from the data warehouse.

*(Creating separate section due to length - would include all 12 insights sections with SQL explanations)*

---

## 6. Dashboard Files

### 6.1 Dashboard Architecture

*(Full dashboard documentation with all components)*

---

## 7. Data Flow

### 7.1 End-to-End Pipeline

```
1. Web Scraping (jumia_scraper.py)
   â”œâ”€ Selenium connects to Jumia Egypt
   â”œâ”€ Searches for product categories
   â”œâ”€ Extracts product data (name, price, rating, etc.)
   â””â”€ Saves to CSV (data/raw/jumia_products_TIMESTAMP.csv)

2. Schema Creation (create_schema.py)
   â”œâ”€ Connects to DuckDB
   â”œâ”€ Creates dim_date table
   â”œâ”€ Creates dim_product table
   â”œâ”€ Creates fact_product_daily table
   â””â”€ Creates auto-increment sequences

3. ETL Processing (etl_pipeline.py)
   â”œâ”€ Extract: Loads all CSV files
   â”œâ”€ Transform: Cleans prices, ratings, calculates discounts
   â”œâ”€ Load: Inserts into dim_product and fact_product_daily
   â””â”€ Verifies row counts

4. Insights Generation (generate_insights.py)
   â”œâ”€ Queries DuckDB warehouse
   â”œâ”€ Generates 12 analytics sections
   â”œâ”€ Formats as Markdown
   â””â”€ Saves to Docs/marketing_insights.md

5. Dashboard (dashboard/app.py)
   â”œâ”€ Connects to DuckDB
   â”œâ”€ Fetches data via SQL queries
   â”œâ”€ Creates interactive visualizations
   â””â”€ Serves on http://localhost:8501
```

### 7.2 Data Transformations

**Raw CSV â†’ Dimensional Model**

| Raw Field | Transformation | Target Field | Table |
|-----------|---------------|--------------|-------|
| product_url | None | product_url | dim_product |
| product_name | None | product_name | dim_product |
| brand | fillna('Unknown') | brand | dim_product |
| search_term | Noneâ†’category | category | dim_product |
| price_egp | clean_price() | price_egp | fact_product_daily |
| old_price | clean_price() | old_price_egp | fact_product_daily |
| discount | strâ†’float | discount_pct | fact_product_daily |
| rating | clean_rating() | rating | fact_product_daily |
| review_count | fillna(0) | review_count | fact_product_daily |
| is_official | fillna(False) | is_official | dim_product |
| is_express | fillna(False) | is_express | dim_product |
| scrape_timestamp | dt.date | date_key | fact_product_daily |

---

## 8. Deployment Guide

### 8.1 First-Time Setup

```bash
# 1. Clone/Download project
cd /path/to/Jumia_Marketing_Data_Pipline

# 2. Build and start services
docker-compose up -d

# 3. Wait for services to be healthy (2-3 minutes)
docker-compose ps

# 4. Access Airflow UI
# URL: http://localhost:8080
# Username: admin
# Password: admin

# 5. Trigger DAG manually
# Click on "jumia_marketing_pipeline" â†’ Toggle ON â†’ Trigger DAG
```

### 8.2 Regular Operations

```bash
# View logs
docker-compose logs -f airflow-scheduler

# Restart a service
docker-compose restart airflow-webserver

# Stop all services
docker-compose down

# Full reset (WARNING: Deletes data)
./reset_project.ps1
```

### 8.3 Running Dashboard

```bash
# Install dependencies (if not using Docker)
pip install -r requirements.txt

# Run dashboard
python -m streamlit run dashboard/app.py

# Access dashboard
# URL: http://localhost:8501
```

---

## 9. Troubleshooting

### 9.1 Common Issues

| Issue | Cause | Solution |
|-------|-------|----------|
| Airflow UI not accessible | Webserver not started | Check `docker-compose ps`, wait for healthy status |
| DAG not appearing | DAG syntax error | Check logs: `docker-compose logs airflow-scheduler` |
| Scraping fails | Selenium not running | Verify `selenium` container is up |
| Database locked | Multiple connections | Close other connections, restart services |
| Empty insights report | No data in warehouse | Run ETL pipeline first |
| Dashboard shows no data | Database missing/empty | Run `python src/etl_pipeline.py` |

### 9.2 Debug Commands

```bash
# Check service health
docker-compose ps

# View logs for specific service
docker-compose logs -f airflow-scheduler

# Execute command in container
docker-compose exec airflow-webserver bash

# Test database connection
python -c "import duckdb; print(duckdb.connect('data/jumia_warehouse.duckdb').execute('SELECT COUNT(*) FROM fact_product_daily').fetchone())"

# List DAGs
docker-compose exec airflow-webserver airflow dags list
```

---

## 10. Appendix

### 10.1 Database Schema Diagram

```sql
-- Full schema with relationships
dim_date (date_key PK)
  â””â”€â”€ fact_product_daily (date_key FK)

dim_product (product_key PK)
  â””â”€â”€ fact_product_daily (product_key FK)

fact_product_daily (fact_id PK)
  â”œâ”€â”€ product_key FK â†’ dim_product
  â”œâ”€â”€ date_key FK â†’ dim_date
  â””â”€â”€ measures: price, rating, reviews, etc.
```

### 10.2 Environment Variables

| Variable | Value | Purpose |
|----------|-------|---------|
| AIRFLOW__DATABASE__SQL_ALCHEMY_CONN | postgresql+psycopg2://... | Airflow metadata DB |
| AIRFLOW__CORE__EXECUTOR | LocalExecutor | Task execution mode |
| AIRFLOW__CORE__FERNET_KEY | 46BKJo... | Encryption key |
| SELENIUM_REMOTE_URL | http://selenium:4444/wd/hub | Selenium Grid endpoint |
| POSTGRES_USER | airflow | PostgreSQL username |
| POSTGRES_PASSWORD | airflow | PostgreSQL password |

### 10.3 Port Mappings

| Port | Service | Purpose |
|------|---------|---------|
| 8080 | Airflow Webserver | DAG management UI |
| 4444 | Selenium Grid | WebDriver endpoint |
| 7900 | Selenium VNC | Browser viewing (VNC) |
| 8501 | Streamlit | Dashboard UI |
| 5432 | PostgreSQL | Database (internal only) |

---

**Document Version:** 1.0  
**Last Updated:** 2025-11-26  
**Contact:** Data Engineering Team

**End of Documentation**
